{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder for anomaly detection",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsqPBFzAvs9p"
      },
      "source": [
        "## AutoEncoder\n",
        "Alberto Bellumat - alberto.bellumat@studenti.unitn.it<br>\n",
        "Claudio Facchinetti - claudio.facchinetti@studenti.unitn.it<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXC7HblZwQsF"
      },
      "source": [
        "### Step 1: Import everything"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTAKSgF89ks3",
        "outputId": "53e8a000-ddbf-4201-9ee9-81722b69eef5"
      },
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps albumentations"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting albumentations\n",
            "  Using cached https://files.pythonhosted.org/packages/e7/27/2fa0ec5e0c04c410cbb54dd79910afa884409440653aa4688654e6497e2a/albumentations-1.0.2-py3-none-any.whl\n",
            "Installing collected packages: albumentations\n",
            "  Found existing installation: albumentations 1.0.2\n",
            "    Uninstalling albumentations-1.0.2:\n",
            "      Successfully uninstalled albumentations-1.0.2\n",
            "Successfully installed albumentations-1.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ9ZDzGxvqoE"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import os\n",
        "from os import listdir\n",
        "import numpy as np\n",
        "import random\n",
        "import shutil\n",
        "import albumentations as A\n",
        "import pandas as pd\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from PIL import Image\n",
        "import gc\n",
        "import filecmp\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGWy2PogPW4q"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUOGPkbm0VmJ",
        "outputId": "403fcc77-dd14-4c89-f420-006165c07122"
      },
      "source": [
        "! rm -rf dataset*\n",
        "! gdown --id 1febPQrnQVsYflQdxXFEqGxNYG7Y1nHH5\n",
        "! unzip -qq dataset.zip -d ./dataset && echo \"Done\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1febPQrnQVsYflQdxXFEqGxNYG7Y1nHH5\n",
            "To: /content/dataset.zip\n",
            "82.9MB [00:00, 178MB/s]\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9TZzyij1dmb"
      },
      "source": [
        "def create_valid(proportion=0.8):\n",
        "    \"\"\"Function for creating the valid folder, which contains the images that will be used as validation set. \n",
        "    During the process, the images for the validation set are moved from the train folder into the valid folder.\n",
        "    The split is done such that all images associated to a person are either in the train or in the valid folder.\n",
        "\n",
        "    Args:\n",
        "        proportion(float): The proportion of unique persons to keep in the train folder.\n",
        "    \"\"\"\n",
        "    \n",
        "    #Create the valid folder\n",
        "    dirName = 'dataset/valid'\n",
        "    if not os.path.exists(dirName):\n",
        "        os.mkdir(dirName)\n",
        "        print(\"Directory \" , dirName ,  \" Created \")\n",
        "    else:    \n",
        "        print(\"Directory \" , dirName ,  \" already exists\")\n",
        "\n",
        "    #Find all unique person IDs.\n",
        "    unique_set = set([x.split('_')[0] for x in os.listdir(\"dataset/train\")])\n",
        "    #Define the number of unique person IDs that will be kept in the train folder.\n",
        "    train_count = int(len(unique_set) * proportion)\n",
        "    list_images = list(unique_set)\n",
        "    #Random shuffle the list of unique IDs\n",
        "    random.shuffle(list_images)\n",
        "    #Split the images for the validation set from the images for the training set, and save the images for the validation set into the valid folder.\n",
        "    train_images = list_images[:train_count]\n",
        "    valid_images = list_images[train_count:]\n",
        "    print(\"Elements in the train folder before \" + str(len(os.listdir(\"dataset/train\"))))\n",
        "    \n",
        "    for image_id in valid_images:\n",
        "        for file in os.listdir(\"dataset/train\"):\n",
        "            if file.startswith(image_id):\n",
        "                shutil.copy(\"dataset/train/\"+file,dirName)\n",
        "                os.remove(\"dataset/train/\"+file)\n",
        "    \n",
        "    \n",
        "    print(\"Elements in the train folder after \" + str(len(os.listdir(\"dataset/train\"))))\n",
        "    print(\"Elements in the valid folder \"+ str(len(os.listdir(\"dataset/valid\"))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DbAN9b_V_Pq"
      },
      "source": [
        "def save_checkpoint(model, optimizer, epoch, val_loss):\n",
        "    \"\"\"Function for storing locally the model into the data.pt file. We also store the optimizer state during the training.\n",
        "\n",
        "    Args:\n",
        "        model: The model that you wish to store.\n",
        "        optimizer: The optimizer you wish to store\n",
        "        epoch: The epoch at wich the store was perfomed.\n",
        "        val_loss: The validation loss computed at the moment in which the storage was perfomed\n",
        "    \"\"\"\n",
        "    torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'epoch': epoch,\n",
        "            'val_loss': val_loss\n",
        "            }, \"data.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckk9j3JkWGbH"
      },
      "source": [
        "def adjust_learning_rate(optimizer, shrink_factor=0.8):\n",
        "    \"\"\"Function for reducing the learning rate of the optimizer.\n",
        "\n",
        "    Args:\n",
        "        optimizer: The optimizer whose learning rate you will reduce.\n",
        "        shrink_factor(float): The proportion of the reduction.\n",
        "    \"\"\"\n",
        "    print(\"\\nDECAYING learning rate.\")\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
        "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZ1lSc2Vy30P",
        "outputId": "9d7ffb0b-18cd-4980-d468-83c57287fa99"
      },
      "source": [
        "#Trasnform used for data augmentation. We apply ranomdly in sequence some horizontal flip, brightnes, gaussian noies, and small rotations. Then we normalzie the images (each pixel is divied by 255)\n",
        "album_transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.6),\n",
        "    A.RandomBrightness(p=0.4, limit=(-0.2, 0.2)),\n",
        "    A.GaussNoise(p=1.0, var_limit=(10.0, 50.0)),\n",
        "    A.Rotate(limit=(-10, 10), border_mode=1),\n",
        "    #Normalize the pixels in the image.\n",
        "    A.Normalize(mean = (0, 0, 0),std  = (1, 1, 1)),\n",
        "    ToTensorV2(),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/transforms.py:1746: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-uBMhhZKAk-"
      },
      "source": [
        "#Transform for converting images to tensor. Not to be used for data augmentation.\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZFydWHGOPGp"
      },
      "source": [
        "class ImagesDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"The ImagesDataset is an extension of the class Dataset of PyTorch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_dir=\"./\", transform=None, aug=1, mode=\"Album\", return_name=False):\n",
        "        \"\"\"\n",
        "\n",
        "        The __init__ method of the class ImagesDataset.\n",
        "        \n",
        "        Args:\n",
        "            img_dir (string): The folder in which the images are present.\n",
        "\n",
        "            transform: The transform you will apply on the image when the method __getitem__ is called.\n",
        "\n",
        "            aug (int): The number of replicas of an image in the original dataset. It should be used in conjuction with the application of a transform for data augmentation.\n",
        "            If aug is set to 2, it means that for each original image in the dataset, we create two replicas (thus the length of the dataset is doubled). Note that these replicas are best used for data augmenation\n",
        "            and the augmentations are only temporary, thus the original dataset is left untouched. \n",
        "\n",
        "            mode(string): It can get the value either \"Album\" or \"Torch\". If \"Torch\" it means that we are using the predfined ones in PyTorch. If \"Album\" it means that the we use the ones we defined using albumentation. \n",
        "\n",
        "        \"\"\"\n",
        "        self.aug = aug\n",
        "        self.return_name=return_name\n",
        "        self.__imgs = []\n",
        "        self.__img_dir = img_dir\n",
        "        self.__trans = transform\n",
        "        self.__mode = mode\n",
        "\n",
        "        if not self.__img_dir.endswith(\"/\"):\n",
        "            self.__img_dir += '/'\n",
        "    \n",
        "        self.__load_img_names()\n",
        "    \n",
        "  \n",
        "    def __len__(self):\n",
        "        return self.aug*len(self.__imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        idx = idx % len(self.__imgs)\n",
        "        img_name = self.__imgs[idx]\n",
        "        img = self.__load_img(img_name)\n",
        "\n",
        "        if self.__trans is not None:\n",
        "            if self.__mode is \"Torch\":\n",
        "                img = self.__trans(img)\n",
        "            elif self.__mode is \"Album\":\n",
        "                transformed = self.__trans(image=np.array(img))\n",
        "                img = transformed[\"image\"]\n",
        "        \n",
        "        if (self.return_name):    \n",
        "            return img, img_name\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "\n",
        "    def __load_img(self, img_name):\n",
        "        img = Image.open(self.__img_dir + img_name)\n",
        "        return img\n",
        "\n",
        "    def __load_img_names(self):\n",
        "        self.__imgs = [x for x in listdir(self.__img_dir)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3YQmf89ytc4"
      },
      "source": [
        "class vgg_brk(nn.Module):\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super(vgg_brk, self).__init__()\n",
        "        self.brk = nn.Sequential(\n",
        "            nn.Conv2d(in_size, out_size, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),  # b, 16, 10, 10\n",
        "            torch.nn.BatchNorm2d(out_size),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_size, out_size, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            torch.nn.BatchNorm2d(out_size),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "        )\n",
        "        \n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.brk(inputs)\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BKAhlWykq2y"
      },
      "source": [
        "class vgg_revbrk(nn.Module):\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super(vgg_revbrk, self).__init__()\n",
        "        self.revbrk = nn.Sequential(\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.BatchNorm2d(in_size),\n",
        "            nn.ConvTranspose2d(in_size, in_size, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.BatchNorm2d(in_size),\n",
        "            nn.ConvTranspose2d(in_size, out_size, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "        )\n",
        "        \n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.revbrk(inputs)\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-koNpjchxb8Y"
      },
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.brk1 = vgg_brk(in_size=3, out_size=64)\n",
        "        self.brk2 = vgg_brk(in_size=64, out_size=128)\n",
        "        self.brk4 = vgg_brk(in_size=128, out_size=256)  \n",
        "        self.maxpooling = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False, return_indices=True)\n",
        "        self.maxunpooling = nn.MaxUnpool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.revbrk1 = vgg_revbrk(in_size=256, out_size=128)\n",
        "        self.revbrk2 = vgg_revbrk(in_size=128, out_size=64)\n",
        "        self.revbrk4 = vgg_revbrk(in_size=64, out_size=3)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.brk1(x)\n",
        "        x, indices1 = self.maxpooling(x)\n",
        "        x = self.brk2(x)\n",
        "        x, indices2 = self.maxpooling(x)\n",
        "        x = self.brk4(x)\n",
        "        x, indices4 = self.maxpooling(x)\n",
        "        x = self.maxunpooling(x, indices4)\n",
        "        x = self.revbrk1(x)\n",
        "        x = self.maxunpooling(x, indices2)\n",
        "        x = self.revbrk2(x)\n",
        "        x = self.maxunpooling(x, indices1)\n",
        "        x = self.revbrk4(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0hZino9DETF"
      },
      "source": [
        "def save_model_to_drive():\n",
        "  drive.mount('/content/gdrive')\n",
        "  model = torch.load('data.pt')\n",
        "  path = F\"/content/gdrive/My Drive/models/data.pt\"\n",
        "  torch.save(model, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwnemjtEaf8T"
      },
      "source": [
        "def train(train_loader, model, optimizer):\n",
        "    \"\"\"Function for the train of the Model in a epoch.\n",
        "\n",
        "    Args:\n",
        "        train_loader: The DataLoader for the training.\n",
        "        model: The model that will be trained.\n",
        "        optimizer: The optimizer used for the training.\n",
        "    \"\"\"\n",
        "  \n",
        "    model.train()\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    running_loss = 0\n",
        "\n",
        "    for data in train_loader:\n",
        "\n",
        "        # print(data.shape[0])\n",
        "\n",
        "        img = data\n",
        "        img = img.to(device)\n",
        "\n",
        "        # ===================forward=====================\n",
        "\n",
        "        output = model(img)\n",
        "        loss = criterion(output, img)\n",
        "\n",
        "        # ===================backward====================\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # ===============================================\n",
        "\n",
        "        running_loss += loss.item() * data.shape[0]\n",
        "\n",
        "    return (running_loss / len(train_loader.sampler))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZlCldXIJQcj"
      },
      "source": [
        "def test(eval_loader, model):\n",
        "    \"\"\"Function for the test of the Model.\n",
        "\n",
        "    Args:\n",
        "        eval_loader: The DataLoader for the testing.\n",
        "        model: The model to be tested.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    criterion = nn.MSELoss()\n",
        "    running_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in eval_loader:\n",
        "            img = data\n",
        "            img = img.to(device)\n",
        "            output = model(img)\n",
        "            loss = criterion(output, img)\n",
        "\n",
        "            running_loss += loss.item() * data.shape[0]\n",
        "\n",
        "    return (running_loss / len(eval_loader.sampler))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj8M0mjCWcEe"
      },
      "source": [
        "def train_model(model, optimizer, starting_epoch=1, patience = 20, batch_size = 32):\n",
        "\n",
        "    \"\"\"Function for the whole training of the Model.\n",
        "\n",
        "    Args:\n",
        "        starting_epoch(int): The epoch from which we will start the training. It is relevant only for a possible resuiming of the training.\n",
        "        optimizer: The optimizer used for the training.\n",
        "        model: The model that will be trained.\n",
        "        patience(int): The number of epochs after which if there are no improvemente the whole training process will be stopped.\n",
        "        batch_size(int): The size of the bacth for the train and test loader\n",
        "    \"\"\"\n",
        "    \n",
        "    #Define the dataset and dataloader for the training. In this part, we will use a transfrom for data augmentation, so anytime you try to access an element of the dataset, the transform will be automatically applied\n",
        "    #on the retrived image. Aug is set to 2, so we have for each original image, two replicas.\n",
        "    train_dataset_full = ImagesDataset(img_dir='./dataset/train/', transform=album_transform, aug=2)\n",
        "    train_loader = DataLoader(train_dataset_full, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    #Define the dataset and datloader for the validation. In this part, no data augmenation will be used.\n",
        "    valid_dataset_full = ImagesDataset(img_dir='./dataset/valid/', transform=img_transform, mode=\"Torch\")\n",
        "    valid_loader = DataLoader(valid_dataset_full, batch_size=batch_size, shuffle=True)\n",
        "    \n",
        "    #the global minimu validation loss. \n",
        "    best_loss = 100000\n",
        "\n",
        "    epoch = starting_epoch\n",
        "  \n",
        "    #counter for the number of epochs without no imrpovements\n",
        "    epochs_with_no_improvements=0\n",
        "    \n",
        "    print('\\nStart training of the AutoEncoder\\n')\n",
        "    \n",
        "    while True:\n",
        "        #In one epoch, train and test the model.\n",
        "        training_loss = train(train_loader, model, optimizer)\n",
        "        validation_loss = test(valid_loader,model)\n",
        "        \n",
        "        print('epoch {}, training loss:{:.4f}, validation loss:{:.4f}'\n",
        "          .format(epoch, training_loss, validation_loss))\n",
        "        \n",
        "        #check if the validation loss computed in this single epoch is less than the global minimum validation loss found on the previous epochs.\n",
        "        #If the validation loss is less than the minumu validation loss, then the validation loss becomes the global minimum validation loss.\n",
        "        is_best = validation_loss < best_loss\n",
        "        best_loss = min(best_loss, validation_loss)\n",
        "        \n",
        "        #if the validation loss is less than the minimu validation loss, then save the model, and set the counter to 0.\n",
        "        if is_best:\n",
        "            save_checkpoint(model,  optimizer, epoch, validation_loss)\n",
        "            print('\\nSave model at epoch {}\\n'.format(epoch))\n",
        "            epochs_with_no_improvements=0\n",
        "\n",
        "        #if the validation loss is not less than the minimum validation loss, then add one to the counter. \n",
        "        #After the addition, if the counter is equal to patience, then stop the trainig, but if it is 8 or 16, then we try to reduce the learning rate of the optimizer.\n",
        "        else:\n",
        "            epochs_with_no_improvements+=1\n",
        "            \n",
        "            if(epochs_with_no_improvements == patience):            \n",
        "                print('\\nEnd of the training at epoch {}'.format(epoch))\n",
        "                break        \n",
        "\n",
        "            elif (epochs_with_no_improvements % 8 == 0):\n",
        "                print('\\nAdjust learning at epoch {}\\n'.format(epoch))\n",
        "                adjust_learning_rate(optimizer, 0.8)\n",
        "\n",
        "        epoch+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMyIs0TdWlq0"
      },
      "source": [
        "def find_anomalies_in_test(model, threshold_loss, batch_size=32):\n",
        "\n",
        "    \"\"\"Function for finding possible junk images in test folder.\n",
        "\n",
        "    Args:\n",
        "        model: The model that will be used.\n",
        "        threshold_loss: The threshold over which an image is considered junk\n",
        "        batch_size: The size of batch for the test loader.\n",
        "    \"\"\"\n",
        "    \n",
        "    test_dataset = ImagesDataset(img_dir='./dataset/test/', transform=img_transform, mode=\"Torch\",return_name=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    model.eval()\n",
        "    criterion = nn.MSELoss(reduction='none')\n",
        "    list_errors = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            img, label = data\n",
        "            img = img.to(device)\n",
        "            output = model(img)\n",
        "            loss_mse = criterion(output, img)\n",
        "        \n",
        "            for i in range(img.shape[0]):\n",
        "                loss= torch.mean(loss_mse[i]).item()\n",
        "                if (loss >= threshold_loss):\n",
        "                    list_errors.append(label[i])\n",
        "                    \n",
        "    return list_errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoXS1pOkSxhr"
      },
      "source": [
        "#IF ACTIVATE TRAINING IS TRUE, THEN THE TRAINING WILL START. THE TRAINING TAKES >10 HOURS IN GOOGLE COLAB, UNLESS YOU STOP IT. IT IS BETTER IF YOU USE THE PRE-TRAINED MODEL.\n",
        "#The pre-trained model is stored in data.pt file.\n",
        "def main(activate_training=False, save_model= False):\n",
        "    \"\"\"Main function for either the training of the AutoEncoder, or the discovery of anomalies.\n",
        "\n",
        "    Args:\n",
        "        activate_training (boolean): A boolean. True if you want to start the training, or False if you want to find the possible junk images in the test dataset.\n",
        "        save_model(boolean): A boolean. Set it to True if you want to save the model in Drive after the training.\n",
        "    \"\"\"\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    if activate_training:\n",
        "        #define the parameters\n",
        "        learning_rate=1e-3 \n",
        "        starting_epoch = 1\n",
        "        #define the model\n",
        "        model = AutoEncoder().to(device)\n",
        "        #define the optimzier\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
        "                             weight_decay=1e-5)\n",
        "        \n",
        "        #create the vlaidation folder.\n",
        "        create_valid()\n",
        "        #train the model\n",
        "        train_model(model, optimizer, starting_epoch)\n",
        "        #and if save_model is true, save the model to Drive\n",
        "        if save_model:\n",
        "          save_model_to_drive()\n",
        "    \n",
        "    else:\n",
        "        #define the model\n",
        "        model = AutoEncoder()\n",
        "        #load the data.\n",
        "        checkpoint = torch.load(\"data.pt\")\n",
        "        #load the model parameters in the data.\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        model.to(device)\n",
        "        #find the list of possible junk images in the test folder\n",
        "        result = find_anomalies_in_test(model, 0.001)\n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A6uM8caWujO",
        "outputId": "187ce51d-b363-438c-bccf-6b773b566e39"
      },
      "source": [
        "#The main() function returns the list of possible junks in the test folder.\n",
        "#From the data in the dataset of the project, you should not be able to identify which image is junk or not, due to the lack of labels for the anomaly detection task.\n",
        "#So, ideally, you would need to manually check that images in the list can be correctly considered anomalies; otherwise, you might risk deleting valid images from the test folder in the next tasks.\n",
        "#But, for simplifying the check process, we \"cheated\" by using as reference the original dataset provided by the following link: http://zheng-lab.cecs.anu.edu.au/Project/project_reid.html.\n",
        "#In the original dataset, the junk images in the test dataset are the ones with the corresponding names starting with \"-1\" or \"00000\".\n",
        "#So, once we found the possible junk images, we perform an image-by-image check, using the names in the original test dataset as labels. For each image in the list, we find its equivalent in the original test dataset, and once we find a match, we check if the matched image has the name starting with either \"-1\" or \"00000\"; we avoid the manual check-in this way.\n",
        "#But, at this pint, you might even directly check image by image using the original test dataset and the test dataset of the project for finding junk images, without relying on the list of possible anomalies; thus, rendering the autoencoder pointless.\n",
        "#Lastly, you need to have the AE saved as data.pt file if you wanna find the list of possible junk images in the test folder.\n",
        "result_anomal = main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:693: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}